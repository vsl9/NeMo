{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo, nemo_asr\n",
    "from nemo_asr.helpers import post_process_predictions\n",
    "import numpy as np\n",
    "import pyaudio as pa\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the checkpoints are available from NGC: https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5\n",
    "MODEL_YAML = '../NeMo-CHECKPOINTS/0.8.2/quartznet15x5/quartznet15x5.yaml'\n",
    "CHECKPOINT_ENCODER = '../NeMo-CHECKPOINTS/0.8.2/quartznet15x5/JasperEncoder-STEP-247400.pt'\n",
    "CHECKPOINT_DECODER = '../NeMo-CHECKPOINTS/0.8.2/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "yaml = YAML(typ=\"safe\")\n",
    "with open(MODEL_YAML) as f:\n",
    "    jasper_model_definition = yaml.load(f)\n",
    "labels = jasper_model_definition['labels']\n",
    "\n",
    "jasper_model_definition['AudioPreprocessing']['dither'] = 0\n",
    "jasper_model_definition['AudioPreprocessing']['pad_to'] = 0\n",
    "jasper_model_definition['AudioPreprocessing']['normalize'] = 'fixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_factory = nemo.core.NeuralModuleFactory(\n",
    "    placement=nemo.core.DeviceType.GPU,\n",
    "    backend=nemo.core.Backend.PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.backends.pytorch.nm import DataLayerNM\n",
    "from nemo.core.neural_types import NeuralType, BatchTag, TimeTag, AxisType\n",
    "import torch\n",
    "\n",
    "class AudioDataLayer(DataLayerNM):\n",
    "    @staticmethod\n",
    "    def create_ports():\n",
    "        input_ports = {}\n",
    "        output_ports = {\n",
    "            \"audio_signal\": NeuralType({0: AxisType(BatchTag),\n",
    "                                        1: AxisType(TimeTag)}),\n",
    "\n",
    "            \"a_sig_length\": NeuralType({0: AxisType(BatchTag)}),\n",
    "        }\n",
    "        return input_ports, output_ports\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        DataLayerNM.__init__(self, **kwargs)\n",
    "        self.output = True\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        self.output = False\n",
    "        return torch.as_tensor(self.signal, dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape, dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signal):\n",
    "        self.signal = np.reshape(signal.astype(np.float32)/32768., [1, -1])\n",
    "        self.signal_shape = np.expand_dims(self.signal.size, 0).astype(np.int64)\n",
    "        self.output = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def data_iterator(self):\n",
    "        return self\n",
    "    \n",
    "# Instantiate necessary neural modules\n",
    "data_layer = AudioDataLayer()\n",
    "\n",
    "data_preprocessor = nemo_asr.AudioPreprocessing(\n",
    "    factory=neural_factory,\n",
    "    **jasper_model_definition[\"AudioPreprocessing\"])\n",
    "\n",
    "jasper_encoder = nemo_asr.JasperEncoder(\n",
    "    feat_in=jasper_model_definition[\"AudioPreprocessing\"][\"features\"],\n",
    "    **jasper_model_definition[\"JasperEncoder\"])\n",
    "\n",
    "jasper_decoder = nemo_asr.JasperDecoderForCTC(\n",
    "    feat_in=jasper_model_definition[\"JasperEncoder\"][\"jasper\"][-1][\"filters\"],\n",
    "    num_classes=len(labels))\n",
    "\n",
    "greedy_decoder = nemo_asr.GreedyCTCDecoder()\n",
    "\n",
    "jasper_encoder.restore_from(CHECKPOINT_ENCODER)\n",
    "jasper_decoder.restore_from(CHECKPOINT_DECODER)\n",
    "\n",
    "# Define inference DAG\n",
    "audio_signal, audio_signal_len = data_layer()\n",
    "processed_signal, processed_signal_len = data_preprocessor(\n",
    "    input_signal=audio_signal,\n",
    "    length=audio_signal_len)\n",
    "encoded, encoded_len = jasper_encoder(audio_signal=processed_signal,\n",
    "                                      length=processed_signal_len)\n",
    "log_probs = jasper_decoder(encoder_output=encoded)\n",
    "predictions = greedy_decoder(log_probs=log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_signal(self, signal):\n",
    "    data_layer.set_signal(signal)\n",
    "    tensors = self.infer([log_probs], verbose=False)\n",
    "    logits = tensors[0][0]\n",
    "    return logits\n",
    "\n",
    "neural_factory.infer_signal = infer_signal.__get__(neural_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Naive softmax implementation for NumPy\n",
    "    '''\n",
    "    m = np.expand_dims(np.max(x, axis=-1), -1)\n",
    "    e = np.exp(x - m)\n",
    "    return e / np.expand_dims(e.sum(axis=-1), -1)\n",
    "\n",
    "\n",
    "class FrameASR:\n",
    "    \n",
    "    def __init__(self, neural_factory, jasper_model_definition,\n",
    "                 frame_len=2, frame_overlap=2.5, \n",
    "                 timestep_duration=0.02, offset=5):\n",
    "        '''\n",
    "        Args:\n",
    "          model_params: list of OpenSeq2Seq arguments (same as for run.py)\n",
    "          scope_name: model's scope name\n",
    "          sr: sample rate, Hz\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          timestep_duration: time per step at model's output, seconds\n",
    "        '''\n",
    "        self.vocab = jasper_model_definition['labels']\n",
    "        self.vocab.append('_')\n",
    "        \n",
    "        self.sr = jasper_model_definition['sample_rate']\n",
    "        self.frame_len = frame_len\n",
    "        self.n_frame_len = int(frame_len * self.sr)\n",
    "        self.frame_overlap = frame_overlap\n",
    "        self.n_frame_overlap = int(frame_overlap * self.sr)\n",
    "        self.n_timesteps_overlap = int(frame_overlap / timestep_duration) - 2\n",
    "        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len, dtype=np.float32)\n",
    "        # self._calibrate_offset()\n",
    "        self.offset = offset\n",
    "        self.reset()\n",
    "        \n",
    "        \n",
    "    def _decode(self, frame, offset=0):\n",
    "        assert len(frame)==self.n_frame_len\n",
    "        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\n",
    "        self.buffer[-self.n_frame_len:] = frame\n",
    "        logits = neural_factory.infer_signal(self.buffer).cpu().numpy()[0]\n",
    "        # print(logits.shape)\n",
    "        decoded = self._greedy_decoder(\n",
    "            logits[self.n_timesteps_overlap:-self.n_timesteps_overlap], \n",
    "            self.vocab\n",
    "        )\n",
    "        return decoded[:len(decoded)-offset]\n",
    "    \n",
    "    def transcribe(self, frame=None, merge=True):\n",
    "        if frame is None:\n",
    "            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\n",
    "        if len(frame) < self.n_frame_len:\n",
    "            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], 'constant')\n",
    "        unmerged = self._decode(frame, self.offset)\n",
    "        if not merge:\n",
    "            return unmerged\n",
    "        return self.greedy_merge(unmerged)\n",
    "    \n",
    "    \n",
    "    def _calibrate_offset(self, wav_file, max_offset=10, n_calib_inter=10):\n",
    "        '''\n",
    "        Calibrate offset for frame-by-frame decoding\n",
    "        '''\n",
    "        sr, signal = wave.read(wav_file)\n",
    "        \n",
    "        # warmup\n",
    "        n_warmup = 1 + int(np.ceil(2.0 * self.frame_overlap / self.frame_len))\n",
    "        for i in range(n_warmup):\n",
    "            decoded = self._decode(signal[self.n_frame_len*i:self.n_frame_len*(i+1)], offset=0)\n",
    "        \n",
    "        i = n_warmup\n",
    "        \n",
    "        offsets = defaultdict(lambda: 0)\n",
    "        while i < n_warmup + n_calib_inter and (i+1)*self.n_frame_len < signal.shape[0]:\n",
    "            decoded_prev = decoded\n",
    "            decoded = self._decode(signal[self.n_frame_len*i:self.n_frame_len*(i+1)], offset=0)\n",
    "            for offset in range(max_offset, 0, -1):\n",
    "                if decoded[:offset] == decoded_prev[-offset:] and decoded[:offset] != ''.join(['_']*offset):\n",
    "                    offsets[offset] += 1\n",
    "                    break\n",
    "            i += 1\n",
    "        self.offset = max(offsets, key=offsets.get)\n",
    "       \n",
    "        \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset frame_history and decoder's state\n",
    "        '''\n",
    "        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\n",
    "        self.prev_char = ''\n",
    "\n",
    "    @staticmethod\n",
    "    def _greedy_decoder(logits, vocab):\n",
    "        s = ''\n",
    "        for i in range(logits.shape[0]):\n",
    "            s += vocab[np.argmax(logits[i])]\n",
    "        return s\n",
    "\n",
    "    def greedy_merge(self, s):\n",
    "        s_merged = ''\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            if s[i] != self.prev_char:\n",
    "                self.prev_char = s[i]\n",
    "                if self.prev_char != '_':\n",
    "                    s_merged += self.prev_char\n",
    "        return s_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_LEN = 0.5\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK_SIZE = int(FRAME_LEN*RATE)\n",
    "\n",
    "asr = FrameASR(neural_factory, jasper_model_definition, frame_len=FRAME_LEN, frame_overlap=2.0, offset=10)\n",
    "asr.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pa.PyAudio()\n",
    "print('Available audio input devices:')\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    if dev.get('maxInputChannels'):\n",
    "        print(i, dev.get('name'))\n",
    "print('Please type input device ID:')\n",
    "dev_idx = int(input())\n",
    "\n",
    "signal = np.zeros(CHUNK_SIZE)\n",
    "empty_counter = 0\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global empty_counter\n",
    "    signal = np.frombuffer(in_data, dtype=np.int16)\n",
    "    text = asr.transcribe(signal)\n",
    "    if len(text):\n",
    "        print(text,end='')\n",
    "        empty_counter = 3\n",
    "    elif empty_counter > 0:\n",
    "        empty_counter -= 1\n",
    "        if empty_counter == 0:\n",
    "            print(' ',end='')\n",
    "    \n",
    "    return (in_data, pa.paContinue)\n",
    "\n",
    "stream = p.open(format=pa.paInt16,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                input_device_index=dev_idx,\n",
    "                stream_callback=callback,\n",
    "                frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "print('Listening...')\n",
    "\n",
    "stream.start_stream()\n",
    "\n",
    "while stream.is_active():\n",
    "    time.sleep(0.1)\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
